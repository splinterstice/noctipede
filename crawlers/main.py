"""Main crawler entry point with enhanced I2P support."""

import sys
import signal
import asyncio
from datetime import datetime

from core import setup_logging, get_logger
from config import get_settings
from database import get_db_manager
from .manager import CrawlerManager


def signal_handler(signum, frame):
    """Handle shutdown signals."""
    logger = get_logger(__name__)
    logger.info("Received shutdown signal, stopping crawler...")
    sys.exit(0)


async def main_async():
    """Main async crawler function."""
    # Setup logging
    setup_logging()
    logger = get_logger(__name__)
    
    # Setup signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    logger.info("ğŸš€ Starting Noctipede Crawler with Enhanced I2P Support")
    
    try:
        # Initialize database
        db_manager = get_db_manager()
        if not db_manager.test_connection():
            logger.error("âŒ Database connection failed")
            return 1
        
        # Create tables if they don't exist
        db_manager.create_tables()
        logger.info("âœ… Database initialized")
        
        # Initialize crawler manager
        crawler_manager = CrawlerManager()
        
        # Load sites to crawl
        sites = crawler_manager.load_sites_from_file()
        if not sites:
            logger.error("âŒ No sites to crawl")
            return 1
        
        logger.info(f"ğŸ“‹ Loaded {len(sites)} sites to crawl")
        
        # Group sites by network type for reporting
        from core import get_network_type
        sites_by_network = {}
        for site in sites:
            network_type = get_network_type(site)
            if network_type not in sites_by_network:
                sites_by_network[network_type] = []
            sites_by_network[network_type].append(site)
        
        for network, network_sites in sites_by_network.items():
            logger.info(f"  ğŸ“¡ {network.upper()}: {len(network_sites)} sites")
        
        # Start crawling with enhanced async support
        logger.info("ğŸ•·ï¸ Starting crawl process...")
        start_time = datetime.now()
        
        results = await crawler_manager.crawl_sites_async(sites)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Report results
        logger.info("=" * 60)
        logger.info("ğŸ¯ CRAWL RESULTS SUMMARY")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Total Sites: {results['total_sites']}")
        logger.info(f"âœ… Successful: {results['successful']}")
        logger.info(f"âŒ Failed: {results['failed']}")
        logger.info(f"â±ï¸ Duration: {duration:.2f} seconds")
        logger.info(f"ğŸ“ˆ Success Rate: {(results['successful'] / results['total_sites'] * 100):.1f}%")
        
        # Detailed results by network type
        network_stats = {}
        for result in results['results']:
            network = result['network_type']
            if network not in network_stats:
                network_stats[network] = {'success': 0, 'failed': 0}
            
            if result['success']:
                network_stats[network]['success'] += 1
            else:
                network_stats[network]['failed'] += 1
        
        logger.info("\nğŸ“¡ Results by Network Type:")
        for network, stats in network_stats.items():
            total = stats['success'] + stats['failed']
            success_rate = (stats['success'] / total * 100) if total > 0 else 0
            logger.info(f"  {network.upper()}: {stats['success']}/{total} ({success_rate:.1f}% success)")
        
        # Show some successful crawls
        successful_results = [r for r in results['results'] if r['success']]
        if successful_results:
            logger.info(f"\nğŸ‰ Sample Successful Crawls:")
            for result in successful_results[:5]:  # Show first 5
                if isinstance(result.get('result'), dict) and result['result'].get('pages'):
                    pages = result['result']['pages']
                    total_content = sum(len(p.get('content', '')) for p in pages)
                    logger.info(f"  âœ… {result['url']} - {len(pages)} pages, {total_content} chars")
        
        logger.info("=" * 60)
        
        return 0 if results['successful'] > 0 else 1
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Crawler interrupted by user")
        return 0
    except Exception as e:
        logger.error(f"âŒ Crawler failed: {e}")
        return 1


def main():
    """Main synchronous entry point."""
    return asyncio.run(main_async())


if __name__ == "__main__":
    sys.exit(main())
